---
---

@string{aps = {American Physical Society,}}


@article{KCC2025,
  abbr={KCC},
  title={Quantile Regression-Based Reward Modeling for Improved Discriminating Human Preferences in Language},
  author={Minseong Kim, Minsik Hong, Dongjae Kim},
  abstract={최근 대규모 언어 모델(Large Language Model, LLM)의 발전은 인간 피드백을 활용한 강화학습(Reinforcement Learning From Human Feedback, RLHF)을 통해 언어 모델의 응답을 인간 선호도에 정렬하는 방향으로 이루어지고 있다. 기존 RLHF 에서는 보상 값을 단일 스칼라 형태로 예측하는 방식을 주로 사용해왔으나 이러한 방식은 보상의 불확실성이나 인간 선호의 다양성을 충분히 반영하지 못하는 한계가 있다. 본 논문에서는 기존 RLHF 에서 사용되는 스칼라 보상 예측 방식의 한계를 극복하기 위해 Quantile Regression 기반의 보상 분포 예측 구조를 도입하였다. 인간 선호 데이터를 기반으로 학습되는 본 보상 모델은 응답에 대한 단일 보상 값이 아닌 분포 형태의 보상 출력을 예측함으로써 보다 세밀한 인간 선호 정보를 포착할 수 있다. 실험에는 HH(Harmlessness–Helpfulness) 응답 비교 데이터셋을 활용하였으며 기존 스칼라 보상 방식과 제안된 분포 방식 간의 정렬 성능을 비교한 결과, 제안된 방식이 인간 선호 정렬 측면에서 더 선호 구분 가능성이 있음을 확인하였다. }, 
  journal={Korea Computer Congress},
  year={2025},
  month={July},
  venue = {domestic conference},
  award={kcc2025_우수논문상.pdf},
  pdf = {kcc2025_paper.pdf},
  selected={true},
}




@article{WDSC2024,
  abbr={WDSC},
  title={Investigation of SQLite Usage and Techniques for Recovering Deleted Records},
  author={Seonghyeon LeeO and Nakyeong Kim and Yeongyu Choi and Minseong Kim and Insoo Lee and Sooyoung Park and Jongmoo Choi},
  abstract={SQLite는 애플리케이션 데이터를 저장하기 위해 광범위하게 사용되는 데이터베이스로 특히 스마트폰 환경에서 최근 몇 년간 꾸준히 높은 사용률을 보인다 이는 포렌식 수사에서 에 저장된 데이터를 . SQLite 살펴볼 필요성을 증가시켰으며 를 사용하는 애플리케이션에 대한 조사와 포렌식 분석을 , SQLite SQLite 위한 등과 같은 도구의 등장을 촉진했다 그러나 기존 Undark, SQLite Deleted Record Parser, FQlite . 의 연구들은 주로 특정 애플리케이션이나 해외 사례에 집중되어 있었으며 다양한 복원 기법들이 실제로 , 어느 정도의 복원율을 보장하는지에 대한 명확한 정보를 제공하지 않았다 본 연구에서는 국내 애플리케 . 이션에서 의 사용 실태를 조사하고 실제 로그에 기반하여 중요성을 확인한다 또한 대표적인 복원 SQLite , . 기술을 선택하여 실제 복원 가능 여부를 검증하고 도구를 선택하여 포렌식 코퍼스에 따른 도구의 , SQLite 복원율 및 프리 페이지 리스트 아티팩트 카빙 과 같은 기법별 복원율 (Free-Page List), (Artifact Carving) 및 성능을 평가한다. (SQLite is a widely used database for storing application data, particularly in smartphone environments, where its usage has remained consistently high in recent years. This prevalence has increased the need to examine data stored in SQLite during forensic investigations, prompting the development of tools like Undark, SQLite Deleted Record Parser, and FQlite for analyzing applications using SQLite and performing forensic analysis. However, existing studies have primarily focused on specific applications or international cases, failing to provide clear information on the recovery rates guaranteed by various recovery techniques. In this study, we investigate the usage of SQLite in domestic applications and verify its significance based on actual logs. Additionally, we select representative recovery techniques to evaluate their feasibility for data recovery and assess the recovery rates of tools using a forensic corpus. We also analyze recovery rates and performance based on specific techniques such as Free-Page List artifact carving.)},
  journal={Workshop on Dependable and Secure Computing},
  year={2024},
  month={August},
  award={WDSC2024_우수논문상2.pdf},
  venue = {domestic conference},
  pdf = {WDSC2024_paper.pdf},
  selected={true},
}


@article{KCC2024,
  abbr={KCC},
  title={Analysis of Performance according to Blob File Size of BlobDB based on RocksDB},
  author={Min Seong Kim and Guangxun Zhao and Gunhee Choi and Jongmoo Choi and SeeHwan Yoo},
  abstract={최근  증가하는  비정형  데이터를  효율적으로  관리하기  위해  키-밸류  형태로  저장되는  데이터베이스의 사용이  늘어나고  있다.  그  중  Meta의  RocksDB는  키-밸류를  더  효율적으로  관리하기  위해  Wisckey에서 제안한  키-밸류  분리  기법을  기반으로  한  BlobDB를  도입하였다.  BlobDB에서는  키만  Compaction하므로 쓰기  증폭이  감소하지만  밸류를  저장하는  Blob  File의  크기에  따라  쓰기  성능의  차이가  발생한다.  본  논문에서는  Blob File의  크기에  따라  발생하는  현상들이  어떤  것들이  있는지  실험을  통해  쓰기  지연과  파일의  Garbage  크기를  보여주고  분석을  통해  향후  연구  방향을  제시하겠다. (The use of key-value databases has been increasing to efficiently manage the growing volume of unstructured data. Among them, Meta's RocksDB has introduced BlobDB, based on the key-value separation technique proposed by Wisckey, to improve key-value management efficiency. In BlobDB, only keys are compacted, which reduces write amplification, but the write performance varies depending on the size of the Blob file used to store values. This paper investigates the effects of Blob file size through experiments, highlighting write latency and garbage size, and provides an analysis to propose directions for future research.)},
  journal={Korea Computer Congress},
  year={2024},
  month={July},
  venue = {domestic conference},
  pdf = {https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11862557},
  selected={true},
}

@article{CV,
  abbr={CV},
  title={kim min seong's cv},
  author={kms},
  abstract={cv},
  journal={cv},
  year={2024},
  month={June},
  venue = {domestic},
  pdf = {CV_김민성.pdf},
  selected={true},
}

@article{award,
  abbr={CV},
  title={hkuthon},
  author={kms},
  abstract={award},
  journal={award},
  year={2025},
  month={May},
  venue = {domestic},
  pdf = {khuthon_award.PDF},
  selected={true},
}